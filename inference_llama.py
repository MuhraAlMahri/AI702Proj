import json
import torch
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm  # è¿›åº¦æ¡åº“

# ğŸ“Œ æŒ‡å®š LLaMA-3.2-1B-Instruct æ¨¡å‹
MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
HF_TOKEN = "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"

# ğŸ“Œ æ£€æŸ¥GPU
if not torch.cuda.is_available():
    raise RuntimeError("éœ€è¦GPUæ”¯æŒï¼")
print(f"ğŸ”§ ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
print(f"ğŸ”§ å¯ç”¨GPUæ•°é‡: {torch.cuda.device_count()}")
print(f"å½“å‰GPUå†…å­˜ä½¿ç”¨æƒ…å†µ: {torch.cuda.memory_allocated(0)/1024**2:.2f} MB")

# ğŸ“Œ åŠ è½½ tokenizer å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME, 
    use_auth_token=HF_TOKEN,
    padding_side="left",  # å·¦ä¾§å¡«å……
    truncation_side="left"  # å·¦ä¾§æˆªæ–­
)
# è®¾ç½®padding token
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME, 
    torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦
    device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ°å¯ç”¨GPU
    use_auth_token=HF_TOKEN
)
# ç¡®ä¿æ¨¡å‹çš„pad_token_idä¸tokenizerä¸€è‡´
model.config.pad_token_id = tokenizer.pad_token_id

# ğŸ“Œ è¯»å–è®­ç»ƒé›†å’Œæµ‹è¯•é›†
with open("train.json", "r") as f:
    train_data = json.load(f)

with open("test.json", "r") as f:
    test_data = json.load(f)

# ğŸ“Œ åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs("output/llama", exist_ok=True)

# ğŸ“Œ å®šä¹‰ä¸åŒæ•°é‡çš„few-shotç¤ºä¾‹ï¼ˆåŒ…æ‹¬0-shotä½œä¸ºbaselineï¼‰
few_shot_counts = [4, 6, 8, 10]

for shot_count in few_shot_counts:
    print(f"\nğŸ”„ å¼€å§‹å¤„ç† {shot_count}-shot ç‰ˆæœ¬")
    
    # æ„å»º few-shot ç¤ºä¾‹
    few_shot_examples = ""
    if shot_count > 0:  # å¦‚æœä¸æ˜¯0-shotï¼Œåˆ™æ·»åŠ ç¤ºä¾‹
        for i in range(shot_count):
            # few_shot_examples += f"Instruction: Analyze the following instructions and determine if they contain contradictions.\n"
            example = train_data[i]
            few_shot_examples += f"Input: {example['input']}\nOutput: {example['output']}\n\n"
        # few_shot_examples += f"Instruction: Analyze the following instructions and determine if they contain contradictions.\n"
    # else:
        # few_shot_examples = "Instruction: Analyze the following instructions and determine if they contain contradictions.\n"

    results = []
    batch_size = 8  # æˆ–æ›´å¤§ï¼Œå–å†³äºæ˜¾å­˜

    # ğŸ“Œ è¿›è¡Œæ¨ç†å¹¶æ˜¾ç¤ºè¿›åº¦
    for i in tqdm(range(0, len(test_data), batch_size), desc=f"Processing {shot_count}-shot"):
        batch = test_data[i:i+batch_size]
        # åœ¨æ¯ä¸ªè¾“å…¥å‰æ·»åŠ instruction
        batch_inputs = tokenizer([few_shot_examples + f"Input: {item['input']}\nOutput:" for item in batch], 
                               return_tensors="pt", 
                               padding=True).to(model.device)
        
        with torch.no_grad():
            batch_outputs = model.generate(
                **batch_inputs,
                max_new_tokens=200,
                temperature=0.2,
                top_p=0.9,
                do_sample=True,
                num_return_sequences=1,
                pad_token_id=tokenizer.eos_token_id,
                use_cache=True  # å¯ç”¨ KV ç¼“å­˜
            )

        # è§£ç æ¨¡å‹è¾“å‡º
        for idx, output in enumerate(batch_outputs):
            response = tokenizer.decode(output, skip_special_tokens=True)
            
            # æå–æ¨¡å‹çš„å®é™…è¾“å‡ºï¼ˆå»æ‰è¾“å…¥éƒ¨åˆ†ï¼‰
            input_text = few_shot_examples + f"Input: {batch[idx]['input']}\nOutput:"
            response = response.replace(input_text, "").strip()

            # å­˜å‚¨ç»“æœ
            results.append({
                "input": batch[idx]["input"],
                "expected_output": batch[idx]["output"],  # çœŸå®ç­”æ¡ˆ
                "model_output": response  # LLaMA ç”Ÿæˆçš„ç­”æ¡ˆ
            })

        # æ¯5æ¡å†™å…¥ä¸€æ¬¡æ–‡ä»¶
        if (i + batch_size) % batch_size == 0:
            output_file = f"output/llama/inference_results_llama_{shot_count}shot.json"
            with open(output_file, "w") as f:
                json.dump(results, f, indent=4)

    # ç¡®ä¿å†™å…¥å‰©ä½™çš„ç»“æœ
    if len(results) % batch_size != 0:
        output_file = f"output/llama/inference_results_llama_{shot_count}shot.json"
        with open(output_file, "w") as f:
            json.dump(results, f, indent=4)

    print(f"âœ… {shot_count}-shot ç‰ˆæœ¬å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° output/llama/inference_results_llama_{shot_count}shot.json")

print("\næ‰€æœ‰ç‰ˆæœ¬å¤„ç†å®Œæˆï¼")