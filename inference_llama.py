import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm  # è¿›åº¦æ¡åº“

# ğŸ“Œ æŒ‡å®š LLaMA-3.2-1B-Instruct æ¨¡å‹
MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
HF_TOKEN = "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"

# ğŸ“Œ åŠ è½½ tokenizer å’Œæ¨¡å‹ï¼ˆç¡®ä¿ä½ æœ‰ Hugging Face è®¿é—®æƒé™ï¼‰
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=HF_TOKEN)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16, device_map="auto", use_auth_token=HF_TOKEN)

# ğŸ“Œ è¯»å–æ•°æ®é›†
with open("test.json", "r") as f:
    test_data = json.load(f)

results = []
batch_size = 5  # æ¯5æ¡å†™å…¥æ–‡ä»¶

# ğŸ“Œ è¿›è¡Œæ¨ç†å¹¶æ˜¾ç¤ºè¿›åº¦
for idx, item in enumerate(tqdm(test_data, desc="Processing")):
    input_text = f"Input: {item['input']}\nOutput:"
    
    # Tokenize
    inputs = tokenizer(input_text, return_tensors="pt").to("cuda")

    # ç”Ÿæˆæ–‡æœ¬
    with torch.no_grad():
        output_ids = model.generate(**inputs, max_new_tokens=200, temperature=0.2, top_p=0.9)
    
    # è§£ç æ¨¡å‹è¾“å‡º
    model_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)

    # å­˜å‚¨ç»“æœ
    results.append({
        "input": item["input"],
        "expected_output": item["output"],  # çœŸå®ç­”æ¡ˆ
        "model_output": model_output  # LLaMA ç”Ÿæˆçš„ç­”æ¡ˆ
    })

    # æ¯5æ¡å†™å…¥ä¸€æ¬¡æ–‡ä»¶
    if (idx + 1) % batch_size == 0:
        with open("inference_results_llama.json", "w") as f:
            json.dump(results, f, indent=4)

# ç¡®ä¿å†™å…¥å‰©ä½™çš„ç»“æœ
if len(results) % batch_size != 0:
    with open("inference_results_llama.json", "w") as f:
        json.dump(results, f, indent=4)

print("âœ… æ¨ç†å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° inference_results_llama.json")