import json
import torch
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from tqdm import tqdm  # å¯¼å…¥ tqdm åº“

# åŠ è½½åŸºç¡€æ¨¡å‹å’Œtokenizer
base_model_name = "Qwen/Qwen2.5-3B-Instruct"

# åŠ è½½tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    base_model_name,
    trust_remote_code=True,
    padding_side="left",
    truncation_side="left"
)
tokenizer.pad_token = tokenizer.eos_token

# åŠ è½½åŸºç¡€æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

# åŠ è½½LoRAé€‚é…å™¨
model = PeftModel.from_pretrained(base_model, "./lora_qwen")
model = model.merge_and_unload()  # åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹

# è¯»å–æ•°æ®é›†
with open("train.json", "r") as f:
    train_data = json.load(f)

with open("test.json", "r") as f:
    test_data = json.load(f)

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs("output/finetuned_qwen", exist_ok=True)

# å®šä¹‰ä¸åŒæ•°é‡çš„few-shotç¤ºä¾‹
few_shot_counts = [8]

# ä½¿ç”¨å›ºå®šçš„éªŒè¯é›†æ¥æ„å»ºfew-shotç¤ºä¾‹
validation_examples = [
    {
        "instruction": "Analyze the following instructions and determine if they contain contradictions.",
        "input": "If you want to go to the store, turn left at the corner. If you want to go to the store, turn right at the corner.",
        "output": "The instructions \"turn left at the corner\" and \"turn right at the corner\" are contradictory. "
    },
    {
        "instruction": "Analyze the following instructions and determine if they contain contradictions.",
        "input": "To make a cake, first mix the flour and sugar. Then add the eggs and milk.",
        "output": "The instructions are consistent and provide a clear sequence of steps."
    },
    {
        "instruction": "Analyze the following instructions and determine if they contain contradictions.",
        "input": "The meeting is at 2 PM. The meeting is at 3 PM.",
        "output": "The instructions \"The meeting is at 2 PM\" and \"The meeting is at 3 PM\" are contradictory. "
    },
    {
        "instruction": "Analyze the following instructions and determine if they contain contradictions.",
        "input": "To open the door, push the handle down. To open the door, pull the handle up.",
        "output": "The instructions \"push the handle down\" and \"pull the handle up\" are contradictory. "
    },
    {
        "instruction": "Analyze the following instructions and determine if they contain contradictions.",
        "input": "To make tea, boil water and add tea leaves. Let it steep for 5 minutes.",
        "output": "The instructions are consistent and provide clear steps."
    }
]

for shot_count in few_shot_counts:
    print(f"\nğŸ”„ å¼€å§‹å¤„ç† {shot_count}-shot ç‰ˆæœ¬")
    
    # æ„å»º few-shot ç¤ºä¾‹
    few_shot_examples = ""
    if shot_count > 0:  # å¦‚æœä¸æ˜¯0-shotï¼Œåˆ™æ·»åŠ ç¤ºä¾‹
        for i in range(shot_count):
            example = validation_examples[i % len(validation_examples)]  # å¾ªç¯ä½¿ç”¨éªŒè¯é›†
            few_shot_examples += f"Input: {example['input']}\nOutput: {example['output']}\n\n"

    results = []
    batch_size = 8  # å¢åŠ æ‰¹å¤„ç†å¤§å°

    # ä½¿ç”¨ tqdm åŒ…è£… test_data ä»¥æ˜¾ç¤ºè¿›åº¦æ¡
    for i in tqdm(range(0, len(test_data), batch_size), desc=f"Processing {shot_count}-shot"):
        batch = test_data[i:i+batch_size]
        
        # æ‰¹é‡å¤„ç†è¾“å…¥
        batch_inputs = tokenizer(
            [few_shot_examples + f"Input: {item['input']}\nOutput:" for item in batch],
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(model.device)

        # æ‰¹é‡ç”Ÿæˆæ–‡æœ¬
        with torch.no_grad():
            batch_outputs = model.generate(
                **batch_inputs,
                max_new_tokens=200,
                temperature=0.2,
                top_p=0.9,
                do_sample=True,
                num_return_sequences=1,
                pad_token_id=tokenizer.eos_token_id,
                use_cache=True  # å¯ç”¨ KV ç¼“å­˜
            )

        # æ‰¹é‡è§£ç æ¨¡å‹è¾“å‡º
        for idx, output in enumerate(batch_outputs):
            response = tokenizer.decode(output, skip_special_tokens=True)
            
            # æå–æ¨¡å‹çš„å®é™…è¾“å‡ºï¼ˆå»æ‰è¾“å…¥éƒ¨åˆ†ï¼‰
            input_text = few_shot_examples + f"Input: {batch[idx]['input']}\nOutput:"
            response = response.replace(input_text, "").strip()

            # å­˜å‚¨ç»“æœ
            results.append({
                "input": batch[idx]["input"],
                "expected_output": batch[idx]["output"],  # çœŸå®ç­”æ¡ˆ
                "model_output": response  # æ¨¡å‹ç”Ÿæˆçš„ç­”æ¡ˆ
            })

        # æ¯å¤„ç†å®Œä¸€ä¸ªæ‰¹æ¬¡å°±å†™å…¥æ–‡ä»¶
        output_file = f"output/finetuned_qwen/inference_results_finetuned_{shot_count}shot.json"
        with open(output_file, "w") as f:
            json.dump(results, f, indent=4)

    print(f"âœ… {shot_count}-shot ç‰ˆæœ¬å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° {output_file}")

print("\næ‰€æœ‰ç‰ˆæœ¬å¤„ç†å®Œæˆï¼") 